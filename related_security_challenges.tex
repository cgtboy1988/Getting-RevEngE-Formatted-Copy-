%\subsection{Security Challenges}
%The use of challenge problems to evaluate the strength of a particular security mechanism has a long history in Computer Security. For example, to show the strength of the RSA cryptosystem, RSA Laboratories put forth a set of factoring challenges with monetary prizes~\cite{weisstein2003rsa}.

%Similarly, the BOSS~\cite{bas2011break} and ALASKA~\cite{alaska} challenges seek to determine the hardness of steganalysis under particular conditions. From the results of the BOSS challenges it was discovered that the top of 142 participants was able to detect its in-question steganography method (the "HUGO" algorithm) around 80\% of the time; the ALASKA challenge is still in progress.  The BOSS Challenge supplied participants with over 9,000 images which had a 50\% change of containing a hidden message. Those with a message had it inserted via the HUGO algorithm, and participants were given the HUGO implementation code as a reference. In both the BOSS and ALASKA challenges, participants were incentivized to participate by giving them data to work with:  assembling data for these types of challenges is a time consuming process, and doing so enabled participants to conduct worthwhile research and eventually publish their methods.

%Challenges have also been used outside of Computer Security to crowd-source novel solutions to difficult problems. Examples of this include the ConLL~\cite{hajivc2009conll} series of challenges and the PASCAL VOC~\cite{everingham2010pascal} challenge. In the ConLL challenges, participants were given sets of natural language samples with various types of annotation and asked to conduct natural language processing tasks. The PASCAL VOC tasked participants with identifying objects in real time video. High quality research resulted from both of these challenges.

%\subsubsection{CTF Events}
%In computer security, capture-the-flag style events are used for education and training by serving participants challenges to solve.  These events can test how well participants can solve security problems using both technical, algorithmic means, and human intuition~\cite{vigna2014ten}. It has been suggested~\cite{taylor2017ctf} that evaluating this human element may be possible by conducting human subject research during capture-the-flag style events. Such competitions have also been leveraged to test symmetric cryptography schemes~\cite{ches2017challenge}.

%\subsubsection{Deobfuscation Challenges}
%\CC{This is the most important subsection. You need to cite the work by Bjorn de Sutter where they used professional red teams, and the work by Tonella and others where they used students. I assume there are other similar studies carried out by the usual suspects. You then need to contrast that with what we're doing. (Maybe you have talked about this elsewhere in the paper, but I think this is a good place to put it.)}

%\CT{This stuff is talked about in 2.3.3.  Should I move that discussion down?  Otherwise it is likely redundant}

%\subsubsection{Tigress Challenges}
%Several challenges generated by the Tigress obfuscator~\cite{tigress} were hosted on the Tigress website~\footnote{See \url{http://tigress.cs.arizona.edu/challenges.html}}. Subjects (anyone on the Internet) were tasked with turning obfuscated code back into plaintext source. Participants were asked to informally  disclose a description of their successful attacks. Several successful submissions were submitted, which also resulted in publications describing the reverse engineering methods used to defeat the obfuscations.  Salwan et al.~\cite{salwan2018symbolic}, for example, employed symbolic execution and taint analysis to defeat the obfuscation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Removed for double blind review
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Here, we extend this work with \revenge, a system with additional challenge sets and more sophisticated data collection and automated submission grading.  Whereas the previous system relied on expert evaluation of the submissions and self reported information for successful submissions, \revenge automatically grades submissions to an extent and utilizes data collection software to show what a participant does to solve a challenge.

%\subsubsection{\revenge Educational Tool}
%Previously, an educational version of \revenge served randomized reverse engineering challenges to students.~\cite{taylor2016tool}  In CTF-style challenges, Burke demonstrated the importance of employing such randomization to deliver unique problems and prevent answer copying.~\cite{burket2015automatic}  We followed recommendations by Schreuders in our randomization efforts---employing parameterization in order to generate multiple problems from the same blueprint with multiple randomly generated seeds.~\cite{schreuders2017security}  However, randomization is not applicable to the current work and has been removed, since the challenges are only assumed to be difficult until any participant solves them.  In fact, randomization in this case would detract from the overall "fairness" of the challenges.

%Besides the removal of randomization, the infrastructure and base, original programs remain similar to the RevEngE Educational Tool.  That is, students have already successfully deobfuscated these types of programs with less complicated obfuscating transforms applied, and thus we consider the programs small and understandable enough to be appropriate for this research.

